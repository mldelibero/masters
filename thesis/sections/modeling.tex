\section {Capacitor Modeling}
\subsection{Statistics Review}
Capacitors can be represented by numerous equivalent-circuit models, which have various degrees of accuracy in representing the reponse in a physical system. Equivalent circuit models for capacitors usually have either individual or groups of terms that are  of interest for different applications. For instance, designers of switch-mode power supplies need to know the ESR of a capacitor in order to be able to use it for output filtering. When an unknown capacitor is characterized, its response to an electrical stimulous will be recorded and then analyzed. The resulting data is fit to a model using regression analysis. Using various statistical analysis tools, one can determine a model's effectiveness at describing a capacitor to either predict its behavior in a circuit or to compare it against other capacitors. This section will provide a brief introduction into the regression analysis techniques that are proposed in this paper to fit capacitor data to a model.

A commonly used regression analysis technique is called the Least Squares Estimate (LSE). It attempts to find a model which minimizes the squared error between an empirical set of data and itself.

\begin{equation}
y = b_0 + b_1 x
\end{equation}

\begin{equation}
E^2 = \sum_{i=1}^{n} (y_i - y)^2
\end{equation}

\begin{equation}
E^2 = \sum_{i=1}^{n} (y_i - (b_0 + b_1 x_i)^2
\end{equation}

\begin{equation}
\frac{\partial E^2}{\partial b_0} = 0 = \sum_{i=1}^{n} (-2y_i +2b_0 + 2b_1 x_i)
\end{equation}

\begin{equation}
\frac{\partial E^2}{\partial b_1} = 0 = \sum_{i=1}^{n} (-2y_i x_i +2b_0 x_i + 2b_1 x_i^2)
\end{equation}

\begin{equation}
\bar{y} = (\sum_{i=1}^{n} y_i) / n
\end{equation}

\begin{equation}
\sum_{i=1}^{n} y_i  = \bar{y}n
\end{equation}

\begin{equation}
0 = (\bar{y} - (b_0 + 2b_1 \bar{x})
\end{equation}

\begin{equation}
0 = (\bar{xy} - (b_0 \bar{x} + 2b_1 \bar{x^2})
\end{equation}

\begin{equation}
b_0 = \bar{y} - b_1 \bar{x}
\end{equation}

\begin{equation}
b_1 = \frac{\bar{xy} - \bar{x}\bar{y}}{\bar{x^2} - \bar{x}^2}
\end{equation}


Levy shows an extension of the simple LSE example that is valid for a genaric polinomial transfer function.

\begin{equation}
G(s) = \frac{A_0 + A_1 s + A_2 s^2 + ... + A_n s^n}{B_0 + B_1 s + B_2 s^2 + ... + B_n s^n}
\end{equation}

\begin{equation}
\lambda _h = \sum_{k=0}^{m} \omega _k ^h
\end{equation}

\begin{equation}
S_h = \sum_{k=0}^{m} \omega _k ^h R_k
\end{equation}

\begin{equation}
T_h = \sum_{k=0}^{m} \omega _k ^h I_k
\end{equation}

\begin{equation}
U_h = \sum_{k=0}^{m} \omega _k ^h (R_k ^2 + I_k ^2)
\end{equation}

\begin{equation}
MN = C
\end{equation}

\begin{equation}
MN = C
\end{equation}

\begin{equation}
M = 
\begin{bmatrix}
\lambda _0 & 0          & -\lambda _2 &  0           & \lambda _4  & \cdots &  T_1    & S_2    & -T_3   & -S_4   &  T_5    & \cdots \\
0          & \lambda _2 & 0           & -\lambda _4  & 0           & \cdots & -S_2    & T_3    &  S_4   & -T_5   & -S_6    & \cdots \\
\lambda _2 & 0          & -\lambda _4 &  0           & \lambda _6  & \cdots &  T_3    & S_4    & -T_5   & -S_6   &  T_7    & \cdots \\
0          & \lambda _4 & 0           & -\lambda _6  & 0           & \cdots & -S_4    & T_5    &  S_6   & -T_7   & -S_8    & \cdots \\

\vdots     & \vdots     &  \vdots     & \vdots       & \vdots      & \vdots &  \vdots & \vdots & \vdots & \vdots &  \vdots & \vdots \\ 
T_1        & -S_2       & -T_3        &  S_4         & T_5         & \cdots &  U_2    & 0      & -U_4   &  0     &  U_6    & \cdots \\
S_2        &  T_3       & -S_4        & -T_5         & S_6         & \cdots &  0      & U_4    &  0     & -U_6   &  0      & \cdots \\
T_3        & -S_4       & -T_5        &  S_6         & T_7         & \cdots &  U_4    & 0      & -U_6   &  0     &  U_8    & \cdots \\
\end{bmatrix}
\end{equation}

\begin{equation}
N = 
\begin{bmatrix}
A_0 \\
A_1 \\
A_2 \\
A_3 \\
\vdots \\
B_1 \\
B_2 \\
B_3 \\\
\Vdots
\end{bmatrix}
\end{equation}

\begin{equation}
C = 
\begin{bmatrix}
S_0 \\
T_1 \\
S_2 \\
T_3 \\
\vdots \\
0   \\
U_2 \\
0   \\\
\Vdots
\end{bmatrix}
\end{equation}

The $R^2$ quantity is roughly a measure of how well the model represents the dataset. Its value is always between 0 and 1, with a number closer to 1 being a better fit. 

wiki -- "A residual is the difference between an observed value and the fitted value provided by a model."

book -- "The difference between the actual observation $y_i$ and the corresponding fitted value $\overline{y_i}$ is the residual. Say $e_i = y_i - \overline{y_i}$."

wiki -- CDF "The cumulative distribution function (CDF) describes the probability that a real-valued random variable X with a given probablity dsitribution will be found to have a value less than or equal to x."
